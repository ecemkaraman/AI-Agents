### **📝 Tokenizers – Converting Text to Model-Readable Data**  

---

## **1️⃣ What is a Tokenizer?**  

💡 **Tokenizers** convert raw text into numerical data that a model can process.  

📌 **Why do we need tokenization?**  
✔️ **Models only process numbers** → Text must be converted into numerical format.  
✔️ **Efficient Representation** → Helps the model understand relationships between words.  
✔️ **Reduces Unknown Words** → Ensures minimal information loss.  

✅ **Example: Raw Text to Tokenized Output**  
```python
text = "Jim Henson was a puppeteer"
tokens = text.split()
print(tokens)
```
📌 **Output:**  
```python
['Jim', 'Henson', 'was', 'a', 'puppeteer']
```
---

## **2️⃣ Types of Tokenization**  

### **🔹 Word-Based Tokenization**  
✔️ **Splits text into words** → Each word assigned a unique ID.  
✔️ **Simple but inefficient** → Large vocabulary & no word relationships.  
✔️ **Challenges**:  
   - "dog" vs. "dogs" → Seen as completely different words.  
   - "run" vs. "running" → No connection between similar words.  
   - **Out-of-Vocabulary (OOV) Issue** → Unknown words are replaced by `[UNK]`.  

✅ **Example: Basic Word Tokenization**  
```python
text = "Jim Henson was a puppeteer"
tokens = text.split()
print(tokens)  # ['Jim', 'Henson', 'was', 'a', 'puppeteer']
```
🚨 **Problem:** Large vocabulary → Too many unique words to track!  

---

### **🔹 Character-Based Tokenization**  
✔️ **Splits text into individual characters** → Smaller vocabulary.  
✔️ **No out-of-vocabulary (OOV) issues** → Every word can be built from characters.  
✔️ **Challenges**:  
   - Less meaningful → Characters alone don’t carry much semantic value.  
   - **Increased sequence length** → "puppeteer" becomes `[p, u, p, p, e, t, e, e, r]`.  

✅ **Example: Character Tokenization**  
```python
text = "Jim"
tokens = list(text)
print(tokens)  # ['J', 'i', 'm']
```
🚨 **Problem:** Too many tokens → Slower processing!  

---

### **🔹 Subword Tokenization (Best of Both Worlds!)**  
✔️ **Frequent words remain whole** → "the", "and", "hello".  
✔️ **Rare words are split into subwords** → "annoyingly" → ["annoying", "ly"].  
✔️ **Efficient representation** → Reduces unknown tokens, minimizes vocabulary size.  

✅ **Example: Subword Tokenization of "Tokenization!"**  
```python
['Token', '##ization', '!']
```
✔️ **Keeps word structure while reducing vocabulary size!**  

---

## **3️⃣ Tokenizer Algorithms Used in NLP Models**  

📌 **Different Transformer models use different tokenization approaches:**  
- **Byte-Pair Encoding (BPE)** → Used in GPT-2.  
- **WordPiece** → Used in BERT.  
- **SentencePiece / Unigram** → Used in multilingual models like mBART.  

---

## **4️⃣ Loading & Saving Tokenizers**  

💡 **Like models, tokenizers can be loaded and saved!**  

✅ **Load Pretrained BERT Tokenizer**  
```python
from transformers import BertTokenizer

tokenizer = BertTokenizer.from_pretrained("bert-base-cased")
```
✅ **Using `AutoTokenizer` (Flexible Approach)**  
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")
```
✅ **Tokenizing a Sentence**  
```python
tokenizer("Using a Transformer network is simple")
```
📌 **Output:**  
```python
{
 'input_ids': [101, 7993, 170, 11303, 1200, 2443, 1110, 3014, 102],
 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0],
 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1]
}
```
🚀 **Explanation:**  
✔️ **`input_ids`** → Tokenized sentence (mapped to vocabulary IDs).  
✔️ **`attention_mask`** → Identifies important tokens (1) vs. padding (0).  
✔️ **`token_type_ids`** → Used in sentence-pair tasks.  

✅ **Saving Tokenizer Locally**  
```python
tokenizer.save_pretrained("my_tokenizer")
```

---

## **5️⃣ Encoding (Tokenization Process Step-by-Step)**  

💡 **Tokenization is a two-step process:**  
1️⃣ **Convert Text → Tokens**  
2️⃣ **Convert Tokens → Input IDs (Numbers)**  

✅ **Example: Tokenization**  
```python
from transformers import AutoTokenizer

tokenizer = AutoTokenizer.from_pretrained("bert-base-cased")

sequence = "Using a Transformer network is simple"
tokens = tokenizer.tokenize(sequence)
print(tokens)
```
📌 **Output:**  
```python
['Using', 'a', 'transform', '##er', 'network', 'is', 'simple']
```
✔️ **"transformer" → Split into ["transform", "##er"]** → Efficient storage!  

✅ **Convert Tokens to Input IDs**  
```python
ids = tokenizer.convert_tokens_to_ids(tokens)
print(ids)
```
📌 **Output:**  
```python
[7993, 170, 11303, 1200, 2443, 1110, 3014]
```
✔️ **Each token is mapped to a numerical ID!**  

---

## **6️⃣ Decoding (Converting Back to Text)**  

💡 **Decoding reverses tokenization** → Converts **IDs back to human-readable text**.  

✅ **Example: Decoding Input IDs**  
```python
decoded_string = tokenizer.decode([7993, 170, 11303, 1200, 2443, 1110, 3014])
print(decoded_string)
```
📌 **Output:**  
```python
'Using a Transformer network is simple'
```
✔️ **Merges subwords back into full words for readability!**  

---

### **🎯 Summary – Key Takeaways**  

✔️ **Types of Tokenizers:**  
   - **Word-based** → Splits at words, large vocabulary, high OOV.  
   - **Character-based** → Splits at characters, tiny vocabulary, inefficient.  
   - **Subword-based** → Best balance of efficiency & meaning.  

✔️ **Loading & Saving Tokenizers:**  
   - **Load:** `AutoTokenizer.from_pretrained("bert-base-cased")`  
   - **Save:** `tokenizer.save_pretrained("dir")`  

✔️ **Processing Steps:**  
   - **Tokenization (`tokenize()`)** → Converts text to tokens.  
   - **Encoding (`convert_tokens_to_ids()`)** → Converts tokens to numbers.  
   - **Decoding (`decode()`)** → Converts numbers back to text.  

🚀 **Next Up: Understanding Attention Mechanisms in Transformers!**
