### **ğŸ¤– Models â€“ Creating, Loading & Using Transformer Models**

---

## **1ï¸âƒ£ Creating a Transformer Model**

ğŸ’¡ **Transformer models** can be initialized from scratch or loaded from a pretrained checkpoint.

ğŸ“Œ **Key Concepts:**

âœ”ï¸ **AutoModel** â†’ Wrapper that auto-detects the correct model architecture.

âœ”ï¸ **Manual Model Selection** â†’ Use the exact class for specific models (e.g., `BertModel`).

âœ… **Example: Initializing a BERT Model from Scratch**

```python
from transformers import BertConfig, BertModel

# Create a configuration
config = BertConfig()

# Initialize model from config (random weights)
model = BertModel(config)

```

ğŸ“Œ **Output Configuration:**

```json
{
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12
}

```

âœ”ï¸ **`hidden_size`** â†’ Dimension of each tokenâ€™s representation.

âœ”ï¸ **`num_hidden_layers`** â†’ Number of Transformer layers.

ğŸš¨ **Warning:** This model is randomly initialized â†’ Needs training before use!

---

## **2ï¸âƒ£ Loading a Pretrained Model**

ğŸ’¡ Instead of training from scratch, we can **load pretrained weights** from the Hugging Face Model Hub.

âœ… **Example: Load Pretrained BERT**

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")

```

ğŸ“Œ **What Happens?**

âœ”ï¸ Downloads pretrained model weights.

âœ”ï¸ Caches model locally (avoids re-downloading).

âœ”ï¸ Ready for **inference or fine-tuning**.

âœ… **Alternative: Use `AutoModel` for Flexibility**

```python
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")

```

âœ”ï¸ **Checkpoint-Agnostic Code** â†’ Works for any model trained on a similar task!

---

## **3ï¸âƒ£ Saving & Reloading a Model**

ğŸ“Œ **To store a model locally, use `save_pretrained()`**

âœ… **Example: Save Model**

```python
model.save_pretrained("my_model_directory")

```

ğŸ“Œ **Saves two files:**

- `config.json` â†’ Stores model architecture details.
- `pytorch_model.bin` â†’ Contains pretrained weights (state dictionary).

âœ… **Example: Reload Model Later**

```python
from transformers import AutoModel

model = AutoModel.from_pretrained("my_model_directory")

```

âœ”ï¸ **Ensures reproducibility & quick reloading!**

---

## **4ï¸âƒ£ Using a Transformer Model for Inference**

ğŸ’¡ **Transformers process only numerical data** â†’ **Text must be tokenized first!**

âœ… **Example: Convert Text to Model Inputs**

```python
sequences = ["Hello!", "Cool.", "Nice!"]

# Tokenized output (input IDs)
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]

```

ğŸ“Œ **Explanation:**

âœ”ï¸ **101 & 102** â†’ Special tokens (CLS & SEP).

âœ”ï¸ **Other numbers** â†’ Token IDs from the modelâ€™s vocabulary.

âœ… **Convert to Tensor (PyTorch Example)**

```python
import torch

model_inputs = torch.tensor(encoded_sequences)

```

âœ… **Run Model Inference**

```python
output = model(model_inputs)

```

âœ”ï¸ **Directly passes tokenized input to model!**

---

### **ğŸ¯ Summary â€“ Key Takeaways**

âœ”ï¸ **Model Initialization:**

- **`BertModel(config)`** â†’ Creates a random model (needs training).
- **`BertModel.from_pretrained("bert-base-cased")`** â†’ Loads pretrained weights.

âœ”ï¸ **Model Storage & Reuse:**

- **Save:** `model.save_pretrained("dir")`
- **Load:** `AutoModel.from_pretrained("dir")`

âœ”ï¸ **Inference Steps:**

- **Tokenize Text** â†’ Convert words to token IDs.
- **Convert to Tensor** â†’ Ensure compatibility with PyTorch/TensorFlow.
- **Pass to Model** â†’ Run inference & obtain predictions.

