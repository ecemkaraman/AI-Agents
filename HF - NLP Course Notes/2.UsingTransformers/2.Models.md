### **🤖 Models – Creating, Loading & Using Transformer Models**

---

## **1️⃣ Creating a Transformer Model**

💡 **Transformer models** can be initialized from scratch or loaded from a pretrained checkpoint.

📌 **Key Concepts:**

✔️ **AutoModel** → Wrapper that auto-detects the correct model architecture.

✔️ **Manual Model Selection** → Use the exact class for specific models (e.g., `BertModel`).

✅ **Example: Initializing a BERT Model from Scratch**

```python
from transformers import BertConfig, BertModel

# Create a configuration
config = BertConfig()

# Initialize model from config (random weights)
model = BertModel(config)

```

📌 **Output Configuration:**

```json
{
  "hidden_size": 768,
  "intermediate_size": 3072,
  "max_position_embeddings": 512,
  "num_attention_heads": 12,
  "num_hidden_layers": 12
}

```

✔️ **`hidden_size`** → Dimension of each token’s representation.

✔️ **`num_hidden_layers`** → Number of Transformer layers.

🚨 **Warning:** This model is randomly initialized → Needs training before use!

---

## **2️⃣ Loading a Pretrained Model**

💡 Instead of training from scratch, we can **load pretrained weights** from the Hugging Face Model Hub.

✅ **Example: Load Pretrained BERT**

```python
from transformers import BertModel

model = BertModel.from_pretrained("bert-base-cased")

```

📌 **What Happens?**

✔️ Downloads pretrained model weights.

✔️ Caches model locally (avoids re-downloading).

✔️ Ready for **inference or fine-tuning**.

✅ **Alternative: Use `AutoModel` for Flexibility**

```python
from transformers import AutoModel

model = AutoModel.from_pretrained("bert-base-cased")

```

✔️ **Checkpoint-Agnostic Code** → Works for any model trained on a similar task!

---

## **3️⃣ Saving & Reloading a Model**

📌 **To store a model locally, use `save_pretrained()`**

✅ **Example: Save Model**

```python
model.save_pretrained("my_model_directory")

```

📌 **Saves two files:**

- `config.json` → Stores model architecture details.
- `pytorch_model.bin` → Contains pretrained weights (state dictionary).

✅ **Example: Reload Model Later**

```python
from transformers import AutoModel

model = AutoModel.from_pretrained("my_model_directory")

```

✔️ **Ensures reproducibility & quick reloading!**

---

## **4️⃣ Using a Transformer Model for Inference**

💡 **Transformers process only numerical data** → **Text must be tokenized first!**

✅ **Example: Convert Text to Model Inputs**

```python
sequences = ["Hello!", "Cool.", "Nice!"]

# Tokenized output (input IDs)
encoded_sequences = [
    [101, 7592, 999, 102],
    [101, 4658, 1012, 102],
    [101, 3835, 999, 102],
]

```

📌 **Explanation:**

✔️ **101 & 102** → Special tokens (CLS & SEP).

✔️ **Other numbers** → Token IDs from the model’s vocabulary.

✅ **Convert to Tensor (PyTorch Example)**

```python
import torch

model_inputs = torch.tensor(encoded_sequences)

```

✅ **Run Model Inference**

```python
output = model(model_inputs)

```

✔️ **Directly passes tokenized input to model!**

---

### **🎯 Summary – Key Takeaways**

✔️ **Model Initialization:**

- **`BertModel(config)`** → Creates a random model (needs training).
- **`BertModel.from_pretrained("bert-base-cased")`** → Loads pretrained weights.

✔️ **Model Storage & Reuse:**

- **Save:** `model.save_pretrained("dir")`
- **Load:** `AutoModel.from_pretrained("dir")`

✔️ **Inference Steps:**

- **Tokenize Text** → Convert words to token IDs.
- **Convert to Tensor** → Ensure compatibility with PyTorch/TensorFlow.
- **Pass to Model** → Run inference & obtain predictions.

