### **ğŸ“Œ Handling Multiple Sequences in Transformer Models**

---

## **1ï¸âƒ£ Why Handle Multiple Sequences?**

ğŸ”¹ Transformer models **expect multiple sequences** as input.

ğŸ”¹ Different sequences **vary in length**, making batching challenging.

ğŸ”¹ Inputs must be in a **rectangular shape (tensor format)** for efficient processing.

ğŸ”¹ Padding and attention masks help manage **variable-length sequences**.

---

## **2ï¸âƒ£ Models Expect a Batch of Inputs**

ğŸ’¡ **Issue:** Sending a single sequence to the model fails.

âœ… **Example: Why does this fail?**

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)  # âŒ Missing batch dimension!
model(input_ids)

```

ğŸš¨ **Error:**

```
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

```

---

## **3ï¸âƒ£ Solution: Add a Batch Dimension**

ğŸ“Œ **Transformers expect batched input.**

âœ”ï¸ **Single Sequence â†’ Needs a batch dimension!**

âœ… **Fix: Add an extra dimension**

```python
input_ids = torch.tensor([ids])  # âœ… Add batch dimension
output = model(input_ids)
print("Logits:", output.logits)

```

ğŸ“Œ **Output:**

```
Logits: [[-2.7276,  2.8789]]

```

---

## **4ï¸âƒ£ Batching Multiple Sequences**

âœ”ï¸ **Batching** allows multiple sequences to be processed together.

âœ”ï¸ **Example:** Duplicate a sequence to create a batch.

âœ… **Creating a batch of two identical sequences**

```python
batched_ids = [ids, ids]  # Batch of two sequences
input_batch = torch.tensor(batched_ids)
output = model(input_batch)
print("Logits:", output.logits)

```

âœï¸ **Try it!** Convert `batched_ids` to a tensor and check that the logits match!

---

## **5ï¸âƒ£ Handling Sequences of Different Lengths (Padding)**

ğŸ’¡ **Problem:** Sequences in a batch must be the same length, but real-world text varies.

ğŸ’¡ **Solution:** **Padding** shorter sequences with a **special padding token**.

âœ… **Example: Unequal Length Sequences**

ğŸš¨ **This won't work!**

```python
batched_ids = [
    [200, 200, 200],  # 3 tokens
    [200, 200]        # 2 tokens (inconsistent length)
]

```

âœ”ï¸ **Fix: Use Padding Token**

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]  # Add padding token
]

```

---

## **6ï¸âƒ£ Example: Padding in Practice**

ğŸ’¡ **Using padding to ensure uniform tensor shape**

âœ… **Example: Padding in Transformers**

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]  # Sequence 1
sequence2_ids = [[200, 200]]       # Sequence 2
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]  # Padded sequence
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)

```

ğŸ“Œ **Output (Incorrect Logits Due to Padding)**

```
tensor([[ 1.5694, -1.3895]],
       [ 1.3373, -1.2163]])

```

ğŸš¨ **Problem:** Padding affects attention layers!

---

## **7ï¸âƒ£ Attention Masks â€“ Ignoring Padding Tokens**

âœ”ï¸ **Attention masks** tell the model to ignore padding tokens.

âœ”ï¸ **1 â†’ Attend to this token**

âœ”ï¸ **0 â†’ Ignore this token**

âœ… **Example: Using Attention Masks**

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]
]

attention_mask = [
    [1, 1, 1],  # Attend to all tokens
    [1, 1, 0],  # Ignore the padding token
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)

```

ğŸ“Œ **Corrected Output (Properly Handling Padding)**

```
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]])

```

âœ”ï¸ **Now the second sequence has the correct logits!**

âœï¸ **Try it!** Apply manual tokenization and padding, and verify correct logits!

---

## **8ï¸âƒ£ Handling Longer Sequences (Truncation)**

ğŸ“Œ **Problem:** Transformer models have a **maximum sequence length** (512 or 1024 tokens).

ğŸ“Œ **Solutions:**

1ï¸âƒ£ Use a **long-sequence model** (e.g., Longformer, LED).

2ï¸âƒ£ **Truncate sequences** to fit within limits.

âœ… **Truncating Long Sequences**

```python
max_sequence_length = 512
truncated_sequence = sequence[:max_sequence_length]

```

---

### **ğŸ¯ Summary â€“ Key Takeaways**

âœ”ï¸ **Transformers expect batched inputs** â†’ Add a **batch dimension**.

âœ”ï¸ **Sequences of different lengths need padding** â†’ Use **padding tokens**.

âœ”ï¸ **Padding affects logits** â†’ Use an **attention mask** to ignore padding.

âœ”ï¸ **Models have a max sequence length** â†’ Use **truncation** if needed.

ğŸš€ **Next Up: Understanding Model Architectures in Transformers!**
