### **📌 Handling Multiple Sequences in Transformer Models**

---

## **1️⃣ Why Handle Multiple Sequences?**

🔹 Transformer models **expect multiple sequences** as input.

🔹 Different sequences **vary in length**, making batching challenging.

🔹 Inputs must be in a **rectangular shape (tensor format)** for efficient processing.

🔹 Padding and attention masks help manage **variable-length sequences**.

---

## **2️⃣ Models Expect a Batch of Inputs**

💡 **Issue:** Sending a single sequence to the model fails.

✅ **Example: Why does this fail?**

```python
import torch
from transformers import AutoTokenizer, AutoModelForSequenceClassification

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence = "I've been waiting for a HuggingFace course my whole life."

tokens = tokenizer.tokenize(sequence)
ids = tokenizer.convert_tokens_to_ids(tokens)
input_ids = torch.tensor(ids)  # ❌ Missing batch dimension!
model(input_ids)

```

🚨 **Error:**

```
IndexError: Dimension out of range (expected to be in range of [-1, 0], but got 1)

```

---

## **3️⃣ Solution: Add a Batch Dimension**

📌 **Transformers expect batched input.**

✔️ **Single Sequence → Needs a batch dimension!**

✅ **Fix: Add an extra dimension**

```python
input_ids = torch.tensor([ids])  # ✅ Add batch dimension
output = model(input_ids)
print("Logits:", output.logits)

```

📌 **Output:**

```
Logits: [[-2.7276,  2.8789]]

```

---

## **4️⃣ Batching Multiple Sequences**

✔️ **Batching** allows multiple sequences to be processed together.

✔️ **Example:** Duplicate a sequence to create a batch.

✅ **Creating a batch of two identical sequences**

```python
batched_ids = [ids, ids]  # Batch of two sequences
input_batch = torch.tensor(batched_ids)
output = model(input_batch)
print("Logits:", output.logits)

```

✏️ **Try it!** Convert `batched_ids` to a tensor and check that the logits match!

---

## **5️⃣ Handling Sequences of Different Lengths (Padding)**

💡 **Problem:** Sequences in a batch must be the same length, but real-world text varies.

💡 **Solution:** **Padding** shorter sequences with a **special padding token**.

✅ **Example: Unequal Length Sequences**

🚨 **This won't work!**

```python
batched_ids = [
    [200, 200, 200],  # 3 tokens
    [200, 200]        # 2 tokens (inconsistent length)
]

```

✔️ **Fix: Use Padding Token**

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]  # Add padding token
]

```

---

## **6️⃣ Example: Padding in Practice**

💡 **Using padding to ensure uniform tensor shape**

✅ **Example: Padding in Transformers**

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint)

sequence1_ids = [[200, 200, 200]]  # Sequence 1
sequence2_ids = [[200, 200]]       # Sequence 2
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]  # Padded sequence
]

print(model(torch.tensor(sequence1_ids)).logits)
print(model(torch.tensor(sequence2_ids)).logits)
print(model(torch.tensor(batched_ids)).logits)

```

📌 **Output (Incorrect Logits Due to Padding)**

```
tensor([[ 1.5694, -1.3895]],
       [ 1.3373, -1.2163]])

```

🚨 **Problem:** Padding affects attention layers!

---

## **7️⃣ Attention Masks – Ignoring Padding Tokens**

✔️ **Attention masks** tell the model to ignore padding tokens.

✔️ **1 → Attend to this token**

✔️ **0 → Ignore this token**

✅ **Example: Using Attention Masks**

```python
batched_ids = [
    [200, 200, 200],
    [200, 200, tokenizer.pad_token_id]
]

attention_mask = [
    [1, 1, 1],  # Attend to all tokens
    [1, 1, 0],  # Ignore the padding token
]

outputs = model(torch.tensor(batched_ids), attention_mask=torch.tensor(attention_mask))
print(outputs.logits)

```

📌 **Corrected Output (Properly Handling Padding)**

```
tensor([[ 1.5694, -1.3895],
        [ 0.5803, -0.4125]])

```

✔️ **Now the second sequence has the correct logits!**

✏️ **Try it!** Apply manual tokenization and padding, and verify correct logits!

---

## **8️⃣ Handling Longer Sequences (Truncation)**

📌 **Problem:** Transformer models have a **maximum sequence length** (512 or 1024 tokens).

📌 **Solutions:**

1️⃣ Use a **long-sequence model** (e.g., Longformer, LED).

2️⃣ **Truncate sequences** to fit within limits.

✅ **Truncating Long Sequences**

```python
max_sequence_length = 512
truncated_sequence = sequence[:max_sequence_length]

```

---

### **🎯 Summary – Key Takeaways**

✔️ **Transformers expect batched inputs** → Add a **batch dimension**.

✔️ **Sequences of different lengths need padding** → Use **padding tokens**.

✔️ **Padding affects logits** → Use an **attention mask** to ignore padding.

✔️ **Models have a max sequence length** → Use **truncation** if needed.

🚀 **Next Up: Understanding Model Architectures in Transformers!**
