### **ğŸ” Behind the Pipeline â€“ How Transformers Work Step by Step**

---

## **1ï¸âƒ£ The Pipeline Process**

ğŸ’¡ The **`pipeline()` function** in ğŸ¤— Transformers simplifies NLP tasks into **three key steps**:

âœ”ï¸ **Preprocessing** â†’ Convert raw text into numerical input using a **tokenizer**.

âœ”ï¸ **Model Inference** â†’ Pass the processed inputs through a **Transformer model**.

âœ”ï¸ **Postprocessing** â†’ Convert model outputs into human-readable predictions.

âœ… **Example: Sentiment Analysis Pipeline**

```python
from transformers import pipeline

classifier = pipeline("sentiment-analysis")
classifier(["I've been waiting for a Hugging Face course my whole life.", "I hate this so much!"])

```

ğŸ“Œ **Output:**

```json
[{'label': 'POSITIVE', 'score': 0.9598},
 {'label': 'NEGATIVE', 'score': 0.9995}]

```

---

## **2ï¸âƒ£ Step 1: Preprocessing with a Tokenizer**

ğŸ“Œ **Why?**

- Transformer models **can't process raw text** â†’ Need conversion into **tokens & numerical inputs**.
- Uses **`AutoTokenizer.from_pretrained()`** to load the correct tokenizer.

âœ… **Example: Tokenizing Sentences**

```python
from transformers import AutoTokenizer

checkpoint = "distilbert-base-uncased-finetuned-sst-2-english"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

raw_inputs = [
    "I've been waiting for a Hugging Face course my whole life.",
    "I hate this so much!",
]
inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors="pt")

```

ğŸ“Œ **Output:**

```json
{
  'input_ids': [[101, 1045, 1005, 2310, ..., 1012, 102], [101, 1045, 5223, ..., 999, 102, 0, 0, 0]],
  'attention_mask': [[1, 1, 1, ..., 1, 1], [1, 1, 1, ..., 1, 1, 0, 0, 0]]
}

```

âœ”ï¸ **`input_ids`** â†’ Tokenized text converted into numerical values.

âœ”ï¸ **`attention_mask`** â†’ Tells the model which tokens to **attend to (1)** and **ignore (0)**.

---

## **3ï¸âƒ£ Step 2: Model Inference (Passing Inputs to the Model)**

ğŸ“Œ **Load a Pretrained Model**

- Uses **`AutoModel.from_pretrained()`** to load the base Transformer model.
- Outputs **hidden states** (high-dimensional contextual representations).

âœ… **Example: Load Model & Run Inference**

```python
from transformers import AutoModel

model = AutoModel.from_pretrained(checkpoint)
outputs = model(**inputs)
print(outputs.last_hidden_state.shape)  # Output shape

```

ğŸ“Œ **Output:**

```json
torch.Size([2, 16, 768])  # (Batch size, Sequence length, Hidden size)

```

âœ”ï¸ **Batch size = 2** â†’ Two input sentences.

âœ”ï¸ **Sequence length = 16** â†’ Tokens per sentence (with padding).

âœ”ï¸ **Hidden size = 768** â†’ Each token is represented as a **768-dimensional vector**.

---

## **4ï¸âƒ£ Step 3: Adding a Model Head for Classification**

ğŸ“Œ **Why?**

- The base model **only generates embeddings**.
- To classify text, we need a **task-specific model head** (e.g., SequenceClassification).

âœ… **Load Model with a Classification Head**

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint)
outputs = model(**inputs)
print(outputs.logits.shape)  # Shape of the output logits

```

ğŸ“Œ **Output:**

```json
torch.Size([2, 2])  # (Batch size, Number of labels)

```

âœ”ï¸ **Two output values per sentence (POSITIVE & NEGATIVE scores).**

---

## **5ï¸âƒ£ Step 4: Postprocessing (Converting Logits to Probabilities)**

ğŸ“Œ **Raw model outputs (logits) are unnormalized â†’ Apply SoftMax to get probabilities.**

âœ… **Example: Convert Logits to Probabilities**

```python
import torch

predictions = torch.nn.functional.softmax(outputs.logits, dim=-1)
print(predictions)

```

ğŸ“Œ **Output:**

```json
tensor([[0.0402, 0.9598],  # Sentence 1: 4% Negative, 96% Positive
        [0.9995, 0.0005]]) # Sentence 2: 99.95% Negative, 0.05% Positive

```

ğŸ“Œ **Map Output to Labels:**

```python
model.config.id2label

```

ğŸ“Œ **Output:**

```json
{0: 'NEGATIVE', 1: 'POSITIVE'}

```

âœ… **Final Predictions:**

- **Sentence 1:** **POSITIVE (96%)**
- **Sentence 2:** **NEGATIVE (99.95%)**

---

### **ğŸ¯ Summary â€“ What Happens Behind the Pipeline?**

1ï¸âƒ£ **Preprocessing** â†’ **Tokenization** (Convert text â†’ token IDs).

2ï¸âƒ£ **Model Inference** â†’ **Pass input through Transformer model** to get hidden states.

3ï¸âƒ£ **Adding a Model Head** â†’ **Convert hidden states to classification scores**.

4ï¸âƒ£ **Postprocessing** â†’ **Apply SoftMax** to get probabilities & map to labels.

âœ… **We have successfully replicated the `pipeline()` function manually!** ğŸš€

---

### **âœï¸ Try It Out!**

1ï¸âƒ£ Choose **two sentences** of your own.

2ï¸âƒ£ Run them through the **sentiment-analysis pipeline**.

3ï¸âƒ£ Manually replicate each step to verify the results! ğŸ”¥
