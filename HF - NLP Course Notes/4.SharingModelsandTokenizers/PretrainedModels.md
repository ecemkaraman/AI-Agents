### **ğŸ“Œ Using Pretrained Models from the Hugging Face Hub**

---

## **1ï¸âƒ£ Overview: Why Use Pretrained Models?**

ğŸ’¡ **The Hugging Face Hub** simplifies the process of:

âœ” **Finding & selecting a model** for your task.

âœ” **Loading models in just a few lines of code**.

âœ” **Switching between different architectures seamlessly**.

âœ” **Contributing & sharing models with the community**.

ğŸš€ **Goal:** Learn how to:

ğŸ”¹ Load and use a pretrained model.

ğŸ”¹ Use the correct model for a given task.

ğŸ”¹ Leverage `AutoModel` for flexibility.

---

## **2ï¸âƒ£ Finding a Model for Your Task**

ğŸ’¡ **Example Task:** Masked Language Modeling (MLM) in **French**.

âœ… **Selecting a Model:**

- We search for **French-based models** for **mask-filling**.
- We select **CamemBERT** (`camembert-base`).
- **Checkpoint Identifier:** `"camembert-base"`.

ğŸ” **Checkpoints are model-specific!** Using the wrong one **may lead to bad results**.

---

## **3ï¸âƒ£ Using a Model with `pipeline()`**

ğŸ’¡ **Simplest way to use a model is via `pipeline()`.**

âœ… **Load & Use the Model**

```python
from transformers import pipeline

# Load the fill-mask pipeline with Camembert
camembert_fill_mask = pipeline("fill-mask", model="camembert-base")

# Run inference
results = camembert_fill_mask("Le camembert est <mask> :)")
print(results)

```

âœ” **Automatically downloads & loads the model**

âœ” **Replaces `<mask>` with the most likely predictions**

âœ… **Example Output**

```python
[
  {'sequence': 'Le camembert est dÃ©licieux :)', 'score': 0.49, 'token': 7200, 'token_str': 'dÃ©licieux'},
  {'sequence': 'Le camembert est excellent :)', 'score': 0.10, 'token': 2183, 'token_str': 'excellent'},
  {'sequence': 'Le camembert est succulent :)', 'score': 0.03, 'token': 26202, 'token_str': 'succulent'}
]

```

âœ” **Top-3 Predictions**: `dÃ©licieux`, `excellent`, `succulent`.

---

## **4ï¸âƒ£ Instantiating a Model Directly**

ğŸ’¡ **Alternative: Load model components separately**

âœ… **Using Model-Specific Classes**

```python
from transformers import CamembertTokenizer, CamembertForMaskedLM

tokenizer = CamembertTokenizer.from_pretrained("camembert-base")
model = CamembertForMaskedLM.from_pretrained("camembert-base")

```

âœ” **Loads tokenizer & model separately**

âœ… **Why Not Use This?**

âŒ **Not flexible** â€“ Only works with Camembert checkpoints.

âŒ **Switching architectures requires changing code.**

---

## **5ï¸âƒ£ Using AutoModel for Flexibility**

ğŸ’¡ **Best Practice: Use `Auto*` classes**

- **Why?** âœ… **AutoModel is architecture-agnostic**
- **Allows easy switching between models.**

âœ… **Load Model with `Auto*` Classes**

```python
from transformers import AutoTokenizer, AutoModelForMaskedLM

# Load tokenizer & model using Auto classes
tokenizer = AutoTokenizer.from_pretrained("camembert-base")
model = AutoModelForMaskedLM.from_pretrained("camembert-base")

```

âœ” **Works with any MLM-compatible model**

âœ” **Easy to switch models (just change checkpoint ID)**

âœ… **Run Inference Manually**

```python
import torch

# Encode input
inputs = tokenizer("Le camembert est <mask> :)", return_tensors="pt")

# Get model predictions
outputs = model(**inputs)

# Extract predicted token
logits = outputs.logits
predicted_token_id = torch.argmax(logits, dim=-1)[0, -2].item()

# Decode output
predicted_word = tokenizer.decode(predicted_token_id)
print(f"Predicted word: {predicted_word}")

```

âœ” **Manually extracts the masked word prediction**

---

## **6ï¸âƒ£ Best Practices When Using Pretrained Models**

âœ… **Check Model Cards**

ğŸ“Œ **Before using a model, check its training details:**

- **Datasets used**
- **Training limitations**
- **Potential biases**

âœ… **Use `AutoModel` for Flexibility**

ğŸš€ **Easier to swap models without rewriting code.**

âœ… **Ensure Model Compatibility**

ğŸ’¡ **Make sure your model is suited for the task**

âŒ **Example of an incorrect use case:**

```python
wrong_pipeline = pipeline("text-classification", model="camembert-base")
# âŒ Wonâ€™t work properly! Camembert is for MLM, not classification.

```

âœ” **Use the task selector on the Hub** to find compatible models.

---

## **ğŸ¯ Summary â€“ Key Takeaways**

âœ” **Hugging Face Hub makes using pretrained models easy**

âœ” **Use `pipeline()` for the simplest implementation**

âœ” **Use `AutoTokenizer` & `AutoModel` for flexibility**

âœ” **Check model documentation for training details & biases**

---

### **ğŸ“Œ Sharing Pretrained Models on the Hugging Face Hub**

---

## **1ï¸âƒ£ Overview: Why Share Models?**

ğŸ’¡ **Sharing models helps the ML community by:**

âœ” **Saving time & compute** â€“ Others can use your model without retraining.

âœ” **Providing reproducibility** â€“ Share fine-tuned models for easy replication.

âœ” **Contributing to open-source AI** â€“ Build on top of existing work.

ğŸš€ **Goal:** Learn how to upload models to the Hugging Face Hub using:

ğŸ”¹ `push_to_hub()` API (Simple & automatic).

ğŸ”¹ `huggingface_hub` Python library (More control).

ğŸ”¹ Web interface (No code required).

ğŸ”¹ Git & Git LFS (Manual but flexible).

---

## **2ï¸âƒ£ Uploading with `push_to_hub()` API (Easiest Method)**

ğŸ”¹ **Step 1: Authenticate to Hugging Face Hub**

```python
from huggingface_hub import notebook_login

notebook_login()

```

âœ” **Alternatively, log in via CLI:**

```bash
huggingface-cli login

```

âœ” **Enter your Hugging Face credentials.**

---

ğŸ”¹ **Step 2: Upload Models During Training**

âœ… If using `Trainer`, enable auto-upload:

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    "bert-finetuned-mrpc",
    save_strategy="epoch",  # Save after every epoch
    push_to_hub=True        # Auto-upload to Hub
)

```

âœ” **Trainer uploads after each epoch**

âœ” **To upload final model:**

```python
trainer.push_to_hub()

```

âœ” **Creates model card with metadata!**

---

ğŸ”¹ **Step 3: Upload Pretrained Models Directly**

âœ… Load a pretrained model:

```python
from transformers import AutoModelForMaskedLM, AutoTokenizer

model = AutoModelForMaskedLM.from_pretrained("camembert-base")
tokenizer = AutoTokenizer.from_pretrained("camembert-base")

```

âœ… **Upload to Hub:**

```python
model.push_to_hub("dummy-model")
tokenizer.push_to_hub("dummy-model")

```

âœ” **Model appears under your Hugging Face profile!**

âœ” **To upload to an organization:**

```python
tokenizer.push_to_hub("dummy-model", organization="huggingface")

```

âœ” **To specify an API token manually:**

```python
tokenizer.push_to_hub("dummy-model", use_auth_token="<TOKEN>")

```

ğŸš€ **Now check your model on:**

ğŸ”— `https://huggingface.co/user-or-organization/dummy-model`

---

## **3ï¸âƒ£ Uploading with `huggingface_hub` Python Library (More Control)**

ğŸ”¹ **Step 1: Install & Authenticate**

```bash
pip install huggingface_hub
huggingface-cli login

```

âœ” **Same login method as `push_to_hub()`**

ğŸ”¹ **Step 2: Create a Model Repository**

âœ… Create a new repo:

```python
from huggingface_hub import create_repo

create_repo("dummy-model")  # Creates repo under your profile

```

âœ… **To create under an organization:**

```python
create_repo("dummy-model", organization="huggingface")

```

âœ… **To make the repo private:**

```python
create_repo("dummy-model", private=True)

```

---

ğŸ”¹ **Step 3: Upload Model Files**

âœ… **Direct upload of config files:**

```python
from huggingface_hub import upload_file

upload_file(
    "<path_to_file>/config.json",
    path_in_repo="config.json",
    repo_id="<namespace>/dummy-model",
)

```

âœ” **Great for small files (<5GB).**

âŒ **Not suitable for large model weights.**

---

## **4ï¸âƒ£ Uploading via Web Interface (No Code Needed)**

âœ… **Go to** [huggingface.co/new](https://huggingface.co/new)

âœ… **Set owner, name, & visibility (public/private).**

âœ… **After creation, manually upload files (README, model weights, etc.).**

âœ… **Edit model card in Markdown format.**

ğŸš€ **Easiest way for non-coders to share models!**

---

## **5ï¸âƒ£ Uploading via Git & Git LFS (Manual but Flexible)**

ğŸ”¹ **Step 1: Install Git LFS & Clone Repo**

```bash
git lfs install
git clone <https://huggingface.co/><namespace>/<model-name>
cd <model-name>

```

âœ” **Creates a local repo for uploading files.**

ğŸ”¹ **Step 2: Save Model & Tokenizer Locally**

```python
model.save_pretrained("<model-folder>")
tokenizer.save_pretrained("<model-folder>")

```

ğŸ”¹ **Step 3: Add & Push Files to Hugging Face**

```bash
git add .
git commit -m "Uploading model files"
git push

```

âœ” **Handles large files automatically via Git LFS.**

---

## **ğŸ¯ Summary â€“ Key Takeaways**

âœ” **Best for Beginners:** `push_to_hub()` (Fastest & simplest).

âœ” **More Control:** `huggingface_hub` library (Repo management & file uploads).

âœ” **No Code Needed:** Web interface (Great for beginners).

âœ” **Power Users:** Git & Git LFS (Full control over repository).