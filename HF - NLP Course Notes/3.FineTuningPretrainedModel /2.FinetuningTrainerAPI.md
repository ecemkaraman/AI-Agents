### **📌 Fine-Tuning a Model Using the Trainer API**

---

## **1️⃣ Overview: Why Use the Trainer API?**

The **🤗 Trainer API** simplifies **fine-tuning pretrained Transformer models** by handling:

✔ **Training loops** (gradient updates, optimizer, scheduler).

✔ **Evaluation & Metrics Calculation** (accuracy, F1, etc.).

✔ **Batching & Dynamic Padding** (via `DataCollatorWithPadding`).

✔ **Saving & Logging** (model checkpoints, logs).

🚀 **Goal:** Fine-tune **BERT (`bert-base-uncased`)** on the **MRPC (Microsoft Research Paraphrase Corpus)** dataset.

---

## **2️⃣ Setting Up the Dataset & Tokenizer**

💡 **Load and Preprocess Dataset**

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

# Load dataset from Hugging Face Hub
raw_datasets = load_dataset("glue", "mrpc")

# Load tokenizer
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Tokenization function
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

# Tokenize the dataset
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# Define data collator for dynamic padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

```

✔ **Efficiently tokenizes & batches data**

✔ **Ensures each batch is padded dynamically**

---

## **3️⃣ Defining Training Arguments**

💡 **Set up `TrainingArguments` to control fine-tuning parameters.**

✅ **Initialize Training Arguments**

```python
from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="test-trainer",  # Where to save model checkpoints
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    save_strategy="epoch",  # Save checkpoint at each epoch
    per_device_train_batch_size=8,  # Batch size for training
    per_device_eval_batch_size=8,  # Batch size for evaluation
    num_train_epochs=3,  # Number of training epochs
    weight_decay=0.01,  # L2 regularization
    logging_dir="logs",  # Where to store logs
    push_to_hub=False,  # Set True if you want to push model to Hugging Face Hub
)

```

✔ **Defines training hyperparameters**

✔ **Enables evaluation & checkpoint saving per epoch**

---

## **4️⃣ Initializing the Model**

💡 **Load a Pretrained Model for Sequence Classification (BERT).**

✅ **Load Model & Define Labels**

```python
from transformers import AutoModelForSequenceClassification

# Load BERT model for binary classification (2 labels)
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

```

✔ **Automatically adapts model for binary classification**

✔ **Replaces pretraining head with a new classification head**

⚠️ **Warning:** A new classification layer is randomly initialized. The model will issue a warning that:

- The **pretraining head is discarded**
- **New classification layers are randomly initialized**
- **Fine-tuning is required!**

---

## **5️⃣ Setting Up the Trainer**

💡 **The `Trainer` API handles training, evaluation, logging, and saving automatically.**

✅ **Initialize Trainer**

```python
from transformers import Trainer

trainer = Trainer(
    model=model,  # Model to train
    args=training_args,  # Training arguments
    train_dataset=tokenized_datasets["train"],  # Training data
    eval_dataset=tokenized_datasets["validation"],  # Validation data
    data_collator=data_collator,  # Handles dynamic padding
    tokenizer=tokenizer,  # Tokenizer used for preprocessing
)

```

✔ **Automatically batches and tokenizes data**

✔ **Handles GPU/TPU acceleration & mixed precision**

---

## **6️⃣ Fine-Tuning the Model**

✅ **Start Training**

```python
trainer.train()

```

✔ **Trains model using gradient updates & optimizer**

✔ **Logs training loss every 500 steps**

✔ **Saves checkpoints at the end of each epoch**

⚠️ **By default, training only logs loss.**

To track **accuracy & F1 score**, we need **evaluation metrics**.

---

## **7️⃣ Adding Custom Evaluation Metrics**

💡 **Define `compute_metrics()` function to evaluate performance.**

✅ **Load Evaluation Metric (Accuracy & F1 Score)**

```python
import evaluate
import numpy as np

# Load GLUE benchmark metrics for MRPC dataset
metric = evaluate.load("glue", "mrpc")

# Define function to compute accuracy & F1
def compute_metrics(eval_preds):
    logits, labels = eval_preds
    predictions = np.argmax(logits, axis=-1)  # Get highest probability class
    return metric.compute(predictions=predictions, references=labels)

```

✔ **Extracts predictions from logits**

✔ **Computes accuracy & F1 score**

✅ **Test Metrics on Validation Set**

```python
predictions = trainer.predict(tokenized_datasets["validation"])
print(predictions.predictions.shape, predictions.label_ids.shape)

# Compute accuracy & F1
print(compute_metrics((predictions.predictions, predictions.label_ids)))

```

✔ **Computes evaluation metrics before integrating them into `Trainer`**

---

## **8️⃣ Training with Automatic Evaluation**

💡 **Modify Trainer to evaluate after each epoch.**

✅ **Reinitialize `Trainer` with `compute_metrics()`**

```python
training_args = TrainingArguments(
    output_dir="test-trainer",
    evaluation_strategy="epoch",  # Evaluate at the end of each epoch
    save_strategy="epoch",
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    num_train_epochs=3,
    weight_decay=0.01,
    logging_dir="logs",
)

# Load a new model to retrain from scratch
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_datasets["train"],
    eval_dataset=tokenized_datasets["validation"],
    data_collator=data_collator,
    tokenizer=tokenizer,
    compute_metrics=compute_metrics,  # Compute Accuracy & F1
)

# Start fine-tuning with evaluation after each epoch
trainer.train()

```

✔ **Automatically evaluates after each epoch**

✔ **Tracks accuracy & F1 score in logs**

---

## **9️⃣ Checking Model Performance**

💡 **After training, check validation accuracy & F1 score.**

✅ **Evaluate Final Model**

```python
final_results = trainer.evaluate()
print(final_results)

```

✔ **Reports final accuracy & F1 score on validation set**

✅ **Example Output (May Vary Slightly Due to Random Initialization)**

```python
{'eval_loss': 0.312,
 'eval_accuracy': 0.8578,
 'eval_f1': 0.8996,
 'epoch': 3.0}

```

✔ **Final model achieves ~85.78% accuracy & 89.96% F1-score**

📌 **Compare with BERT Paper:**

- The BERT Base model reported **88.9 F1-score**
- Our model achieves **comparable performance (~89.96 F1)** 🎯

---

## **🔟 Next Steps & Optimization**

💡 **Boost performance using additional Trainer settings:**

✅ **Enable `fp16` for Mixed Precision Training (Faster & More Efficient)**

```python
training_args = TrainingArguments(
    output_dir="test-trainer",
    evaluation_strategy="epoch",
    save_strategy="epoch",
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    num_train_epochs=4,  # Increase epochs
    weight_decay=0.01,
    logging_dir="logs",
    fp16=True,  # Use Mixed Precision Training
)

```

✅ **Use Multi-GPU or TPU for Faster Training**

```python
import torch
torch.cuda.is_available()  # Check if GPU is available

```

✔ **Trainer automatically scales across multiple GPUs/TPUs**

✅ **Push Model to the Hugging Face Hub**

```python
trainer.push_to_hub()

```

✔ **Uploads trained model to Hugging Face Model Hub**

---

## **🎯 Summary – Key Takeaways**

✔ **Trainer API simplifies fine-tuning** → Handles batching, padding, optimizer, logging.

✔ **Evaluation metrics (Accuracy & F1) are added via `compute_metrics()`**.

✔ **Model achieves ~85.78% accuracy & 89.96% F1 on MRPC**.

✔ **Trainer supports GPU/TPU acceleration & mixed precision (`fp16`).**

✔ **Model can be pushed to Hugging Face Hub for public use!** 🚀

---

🔥 **Next:** Fine-tuning in **pure PyTorch** without the Trainer API! 🚀
