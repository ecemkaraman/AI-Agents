### **📌 Full Training Pipeline Without `Trainer` API**

---

## **1️⃣ Why Train Without the Trainer API?**

While the `Trainer` API **simplifies fine-tuning**, sometimes you need **full control** over:

✔ Custom **training loops**

✔ **Gradient accumulation** for large models

✔ Advanced **distributed training** setups

🚀 **Goal:** Fine-tune **BERT (`bert-base-uncased`)** on the **MRPC dataset** (paraphrase detection).

---

## **2️⃣ Data Preparation & Tokenization**

💡 **Load dataset & preprocess using Hugging Face `datasets`.**

✅ **Load and Tokenize the Dataset**

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

# Load MRPC dataset
raw_datasets = load_dataset("glue", "mrpc")

# Load tokenizer
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Tokenization function
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

# Tokenize dataset
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# Define dynamic padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

```

✔ **Tokenizes sentence pairs**

✔ **Applies dynamic padding for efficiency**

✅ **Remove Unused Columns & Convert to PyTorch Format**

```python
# Remove unused columns
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])

# Rename label column to "labels" (expected by model)
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# Convert to PyTorch tensors
tokenized_datasets.set_format("torch")

```

✔ **Ensures only required columns are kept**

✔ **Renames `label` to `labels` for compatibility**

---

## **3️⃣ Creating Dataloaders for Training & Evaluation**

💡 **Use PyTorch `DataLoader` for batch processing.**

✅ **Create Dataloaders**

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)

```

✔ **Handles shuffling & dynamic padding**

✔ **Optimized for PyTorch training loop**

✅ **Inspect a Sample Batch**

```python
for batch in train_dataloader:
    break

{k: v.shape for k, v in batch.items()}

```

✔ **Verifies batch size & tensor shapes**

---

## **4️⃣ Model Initialization**

💡 **Load `BERT` for sequence classification.**

✅ **Load Pretrained Model**

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

```

✔ **Uses BERT with a new classification head**

✅ **Test Forward Pass**

```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)

```

✔ **Confirms model outputs loss & logits**

---

## **5️⃣ Optimizer & Learning Rate Scheduler**

💡 **Use AdamW optimizer & linear decay scheduler.**

✅ **Define Optimizer**

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

```

✔ **Uses `AdamW` (better weight decay)**

✅ **Define Learning Rate Scheduler**

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

```

✔ **Decays learning rate linearly**

✅ **Check Total Training Steps**

```python
print(num_training_steps)

```

✔ **Verifies correct number of updates**

---

## **6️⃣ Training Loop in PyTorch**

💡 **Enable GPU acceleration & define the loop.**

✅ **Move Model to GPU (if available)**

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

```

✔ **Ensures GPU acceleration**

✅ **Define Training Loop**

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

```

✔ **Handles backpropagation & updates**

✔ **Uses `tqdm` for progress tracking**

---

## **7️⃣ Evaluation Loop**

💡 **Calculate Accuracy & F1 Score using `🤗 Evaluate`.**

✅ **Define Evaluation Loop**

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()

for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

# Compute final accuracy & F1-score
print(metric.compute())

```

✔ **Computes batch-wise metrics**

✔ **Returns final accuracy & F1 score**

✅ **Example Output (May Vary Due to Random Initialization)**

```python
{'accuracy': 0.8431, 'f1': 0.8907}

```

✔ **Comparable to results using `Trainer` API**

---

## **8️⃣ 🚀 Speeding Up Training with 🤗 Accelerate**

💡 **Enable multi-GPU/TPU training with minimal changes.**

✅ **Modify Training Loop with `Accelerate`**

```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

# Initialize Accelerator
accelerator = Accelerator()

# Load Model & Optimizer
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

# Prepare for distributed training
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

# Define Scheduler
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

# Training Loop
progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)  # Uses Accelerate for backprop

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

```

✔ **Automatically scales to multi-GPU or TPU**

✔ **Simplifies distributed training**

✅ **Run in a Distributed Setup**

```bash
accelerate config
accelerate launch train.py

```

✔ **Auto-configures multi-device training**

✅ **Run in a Jupyter Notebook (TPU Support)**

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)

```

✔ **Allows TPU training in Colab**

---

## **🎯 Summary – Key Takeaways**

✔ **Full training without `Trainer` API for more control**

✔ **Evaluation using `🤗 Evaluate` for accuracy & F1**

✔ **Accelerated training with `🤗 Accelerate` for multi-GPU/TPU**

✔ **Performance comparable to `Trainer` API (~84.3% Accuracy, ~89.0% F1)**

---

🔥 **Next:** Optimizing for **low-memory environments & longer sequence training!** 🚀
