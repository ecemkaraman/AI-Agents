### **ğŸ“Œ Full Training Pipeline Without `Trainer` API**

---

## **1ï¸âƒ£ Why Train Without the Trainer API?**

While the `Trainer` API **simplifies fine-tuning**, sometimes you need **full control** over:

âœ” Custom **training loops**

âœ” **Gradient accumulation** for large models

âœ” Advanced **distributed training** setups

ğŸš€ **Goal:** Fine-tune **BERT (`bert-base-uncased`)** on the **MRPC dataset** (paraphrase detection).

---

## **2ï¸âƒ£ Data Preparation & Tokenization**

ğŸ’¡ **Load dataset & preprocess using Hugging Face `datasets`.**

âœ… **Load and Tokenize the Dataset**

```python
from datasets import load_dataset
from transformers import AutoTokenizer, DataCollatorWithPadding

# Load MRPC dataset
raw_datasets = load_dataset("glue", "mrpc")

# Load tokenizer
checkpoint = "bert-base-uncased"
tokenizer = AutoTokenizer.from_pretrained(checkpoint)

# Tokenization function
def tokenize_function(example):
    return tokenizer(example["sentence1"], example["sentence2"], truncation=True)

# Tokenize dataset
tokenized_datasets = raw_datasets.map(tokenize_function, batched=True)

# Define dynamic padding
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

```

âœ” **Tokenizes sentence pairs**

âœ” **Applies dynamic padding for efficiency**

âœ… **Remove Unused Columns & Convert to PyTorch Format**

```python
# Remove unused columns
tokenized_datasets = tokenized_datasets.remove_columns(["sentence1", "sentence2", "idx"])

# Rename label column to "labels" (expected by model)
tokenized_datasets = tokenized_datasets.rename_column("label", "labels")

# Convert to PyTorch tensors
tokenized_datasets.set_format("torch")

```

âœ” **Ensures only required columns are kept**

âœ” **Renames `label` to `labels` for compatibility**

---

## **3ï¸âƒ£ Creating Dataloaders for Training & Evaluation**

ğŸ’¡ **Use PyTorch `DataLoader` for batch processing.**

âœ… **Create Dataloaders**

```python
from torch.utils.data import DataLoader

train_dataloader = DataLoader(
    tokenized_datasets["train"], shuffle=True, batch_size=8, collate_fn=data_collator
)
eval_dataloader = DataLoader(
    tokenized_datasets["validation"], batch_size=8, collate_fn=data_collator
)

```

âœ” **Handles shuffling & dynamic padding**

âœ” **Optimized for PyTorch training loop**

âœ… **Inspect a Sample Batch**

```python
for batch in train_dataloader:
    break

{k: v.shape for k, v in batch.items()}

```

âœ” **Verifies batch size & tensor shapes**

---

## **4ï¸âƒ£ Model Initialization**

ğŸ’¡ **Load `BERT` for sequence classification.**

âœ… **Load Pretrained Model**

```python
from transformers import AutoModelForSequenceClassification

model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)

```

âœ” **Uses BERT with a new classification head**

âœ… **Test Forward Pass**

```python
outputs = model(**batch)
print(outputs.loss, outputs.logits.shape)

```

âœ” **Confirms model outputs loss & logits**

---

## **5ï¸âƒ£ Optimizer & Learning Rate Scheduler**

ğŸ’¡ **Use AdamW optimizer & linear decay scheduler.**

âœ… **Define Optimizer**

```python
from transformers import AdamW

optimizer = AdamW(model.parameters(), lr=5e-5)

```

âœ” **Uses `AdamW` (better weight decay)**

âœ… **Define Learning Rate Scheduler**

```python
from transformers import get_scheduler

num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)

lr_scheduler = get_scheduler(
    "linear",
    optimizer=optimizer,
    num_warmup_steps=0,
    num_training_steps=num_training_steps,
)

```

âœ” **Decays learning rate linearly**

âœ… **Check Total Training Steps**

```python
print(num_training_steps)

```

âœ” **Verifies correct number of updates**

---

## **6ï¸âƒ£ Training Loop in PyTorch**

ğŸ’¡ **Enable GPU acceleration & define the loop.**

âœ… **Move Model to GPU (if available)**

```python
import torch

device = torch.device("cuda") if torch.cuda.is_available() else torch.device("cpu")
model.to(device)

```

âœ” **Ensures GPU acceleration**

âœ… **Define Training Loop**

```python
from tqdm.auto import tqdm

progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        batch = {k: v.to(device) for k, v in batch.items()}
        outputs = model(**batch)
        loss = outputs.loss
        loss.backward()

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

```

âœ” **Handles backpropagation & updates**

âœ” **Uses `tqdm` for progress tracking**

---

## **7ï¸âƒ£ Evaluation Loop**

ğŸ’¡ **Calculate Accuracy & F1 Score using `ğŸ¤— Evaluate`.**

âœ… **Define Evaluation Loop**

```python
import evaluate

metric = evaluate.load("glue", "mrpc")
model.eval()

for batch in eval_dataloader:
    batch = {k: v.to(device) for k, v in batch.items()}
    with torch.no_grad():
        outputs = model(**batch)

    logits = outputs.logits
    predictions = torch.argmax(logits, dim=-1)
    metric.add_batch(predictions=predictions, references=batch["labels"])

# Compute final accuracy & F1-score
print(metric.compute())

```

âœ” **Computes batch-wise metrics**

âœ” **Returns final accuracy & F1 score**

âœ… **Example Output (May Vary Due to Random Initialization)**

```python
{'accuracy': 0.8431, 'f1': 0.8907}

```

âœ” **Comparable to results using `Trainer` API**

---

## **8ï¸âƒ£ ğŸš€ Speeding Up Training with ğŸ¤— Accelerate**

ğŸ’¡ **Enable multi-GPU/TPU training with minimal changes.**

âœ… **Modify Training Loop with `Accelerate`**

```python
from accelerate import Accelerator
from transformers import AdamW, AutoModelForSequenceClassification, get_scheduler

# Initialize Accelerator
accelerator = Accelerator()

# Load Model & Optimizer
model = AutoModelForSequenceClassification.from_pretrained(checkpoint, num_labels=2)
optimizer = AdamW(model.parameters(), lr=3e-5)

# Prepare for distributed training
train_dataloader, eval_dataloader, model, optimizer = accelerator.prepare(
    train_dataloader, eval_dataloader, model, optimizer
)

# Define Scheduler
num_epochs = 3
num_training_steps = num_epochs * len(train_dataloader)
lr_scheduler = get_scheduler(
    "linear", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps
)

# Training Loop
progress_bar = tqdm(range(num_training_steps))

model.train()
for epoch in range(num_epochs):
    for batch in train_dataloader:
        outputs = model(**batch)
        loss = outputs.loss
        accelerator.backward(loss)  # Uses Accelerate for backprop

        optimizer.step()
        lr_scheduler.step()
        optimizer.zero_grad()
        progress_bar.update(1)

```

âœ” **Automatically scales to multi-GPU or TPU**

âœ” **Simplifies distributed training**

âœ… **Run in a Distributed Setup**

```bash
accelerate config
accelerate launch train.py

```

âœ” **Auto-configures multi-device training**

âœ… **Run in a Jupyter Notebook (TPU Support)**

```python
from accelerate import notebook_launcher

notebook_launcher(training_function)

```

âœ” **Allows TPU training in Colab**

---

## **ğŸ¯ Summary â€“ Key Takeaways**

âœ” **Full training without `Trainer` API for more control**

âœ” **Evaluation using `ğŸ¤— Evaluate` for accuracy & F1**

âœ” **Accelerated training with `ğŸ¤— Accelerate` for multi-GPU/TPU**

âœ” **Performance comparable to `Trainer` API (~84.3% Accuracy, ~89.0% F1)**

---

ğŸ”¥ **Next:** Optimizing for **low-memory environments & longer sequence training!** ğŸš€
