## **3 Transformer Models**

| **Model Type** | **Examples** | **Best For** |
| --- | --- | --- |
| **📥 Encoder-only** | ALBERT, BERT, DistilBERT, ELECTRA, RoBERTa | **Understanding text** → Sentence classification, NER, Extractive QA |
| **📤 Decoder-only** | CTRL, GPT, GPT-2, Transformer XL | **Text Generation** → Autoregressive tasks (story writing, chatbots) |
| **🔄 Seq2Seq** | BART, T5, Marian, mBART | **Text Transformation** → Summarization, Translation, Generative QA |

---

### **📥 Encoder-Only Transformer Models**

💡 **Encoder models** use only the **encoder** part of a Transformer model.

📌 **Key Features:**

- **🔄 Bi-Directional Attention** → Can see **both left & right** context.
- **📖 Auto-Encoding Models** → Focus on **understanding** entire sentences.
- **⚙️ Training Method** → Corrupt input text (e.g., **masking words**) → Train model to **reconstruct** it.

✅ **Best for tasks that require full-sentence comprehension!**

📌 **Ideal for:**

✔️ **Sentence Classification** → Sentiment analysis, spam detection.

✔️ **Named Entity Recognition (NER)** → Identifying people, locations, organizations.

✔️ **Word-Level Classification** → Part-of-Speech (POS) tagging.

✔️ **Extractive Question Answering** → Finding answers from a given text.

🔥 **Popular Encoder Models:**

- **📖 BERT** → Original bi-directional Transformer.
- **🧠 RoBERTa** → Improved BERT (longer training, more data).
- **⚡ DistilBERT** → **Smaller, faster, cheaper** version of BERT.
- **🧩 ALBERT** → Efficient BERT variant (parameter sharing).
- **⚙️ ELECTRA** → **Replaces words instead of masking** for better efficiency.

---

### **📤 Decoder-Only Transformer Models**

💡 **Decoder models** use only the **decoder** part of a Transformer model.

📌 **Key Features:**

- **🔄 Auto-Regressive** → Predicts text **one word at a time**, only seeing past words.
- **📝 Next-Word Prediction** → Trained by **guessing the next word** in a sentence.

✅ **Best for tasks that require text generation!**

📌 **Ideal for:**

✔️ **Text Generation** → Writing articles, stories, or code.

✔️ **Chatbots & Conversational AI** → Open-domain conversations.

✔️ **Autoregressive Completion** → Completing user input (e.g., autocomplete).

🔥 **Popular Decoder Models:**

- **🛠 GPT** → Original auto-regressive Transformer.
- **🚀 GPT-2** → Larger, better version of GPT.
- **📈 Transformer XL** → Long-context text generation.
- **🔄 CTRL** → Controlled text generation with predefined prompts.

---

### **🔄 Sequence-to-Sequence (Seq2Seq) Transformer Models**

💡 **Seq2Seq models (encoder-decoder models)** use **both** the encoder & decoder components of a Transformer.

📌 **Key Features:**

- **📥 Encoder** → **Processes** the full input sentence.
- **📤 Decoder** → **Generates output** word-by-word, attending only to previous words.
- **📝 Advanced Pretraining** → Can use **both** encoder & decoder objectives or custom training (e.g., T5 replaces text spans with a single mask).

✅ **Best for tasks that require generating text based on an input sentence!**

📌 **Ideal for:**

✔️ **Summarization** → Condensing long text into key points.

✔️ **Translation** → Converting text between languages.

✔️ **Generative Question Answering** → Answering questions by generating responses.

🔥 **Popular Seq2Seq Models:**

- **📄 BART** → Advanced text-to-text generation.
- **🌍 mBART** → Multilingual BART for translation.
- **🔁 Marian** → Specialized in machine translation.
- **🧠 T5** → Converts **every NLP task into a text-to-text format**.
