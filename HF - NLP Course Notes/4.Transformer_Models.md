### **📥 Encoder-Only Transformer Models**

---

## **1️⃣ What Are Encoder Models?**

💡 **Encoder models** use only the **encoder** part of a Transformer model.

📌 **Key Features:**

- **🔄 Bi-Directional Attention** → Can see **both left & right** context.
- **📖 Auto-Encoding Models** → Focus on **understanding** entire sentences.
- **⚙️ Training Method** → Corrupt input text (e.g., **masking words**) → Train model to **reconstruct** it.

✅ **Best for tasks that require full-sentence comprehension!**

---

## **2️⃣ What Are Encoder Models Used For?**

📌 **Ideal for:**

✔️ **Sentence Classification** → Sentiment analysis, spam detection.

✔️ **Named Entity Recognition (NER)** → Identifying people, locations, organizations.

✔️ **Word-Level Classification** → Part-of-Speech (POS) tagging.

✔️ **Extractive Question Answering** → Finding answers from a given text.

---

## **3️⃣ Popular Encoder Models**

🔥 **Well-known encoder-based Transformers:**

- **📖 BERT** → Original bi-directional Transformer.
- **🧠 RoBERTa** → Improved BERT (longer training, more data).
- **⚡ DistilBERT** → **Smaller, faster, cheaper** version of BERT.
- **🧩 ALBERT** → Efficient BERT variant (parameter sharing).
- **⚙️ ELECTRA** → **Replaces words instead of masking** for better efficiency.

---

### **📤 Decoder-Only Transformer Models**

---

## **1️⃣ What Are Decoder Models?**

💡 **Decoder models** use only the **decoder** part of a Transformer model.

📌 **Key Features:**

- **🔄 Auto-Regressive** → Predicts text **one word at a time**, only seeing past words.
- **📝 Next-Word Prediction** → Trained by **guessing the next word** in a sentence.

✅ **Best for tasks that require text generation!**

---

## **2️⃣ What Are Decoder Models Used For?**

📌 **Ideal for:**

✔️ **Text Generation** → Writing articles, stories, or code.

✔️ **Chatbots & Conversational AI** → Open-domain conversations.

✔️ **Autoregressive Completion** → Completing user input (e.g., autocomplete).

---

## **3️⃣ Popular Decoder Models**

🔥 **Well-known decoder-based Transformers:**

- **🛠 GPT** → Original auto-regressive Transformer.
- **🚀 GPT-2** → Larger, better version of GPT.
- **📈 Transformer XL** → Long-context text generation.
- **🔄 CTRL** → Controlled text generation with predefined prompts.

---

### **🔄 Sequence-to-Sequence (Seq2Seq) Transformer Models**

---

## **1️⃣ What Are Sequence-to-Sequence Models?**

💡 **Seq2Seq models (encoder-decoder models)** use **both** the encoder & decoder components of a Transformer.

📌 **Key Features:**

- **📥 Encoder** → **Processes** the full input sentence.
- **📤 Decoder** → **Generates output** word-by-word, attending only to previous words.
- **📝 Advanced Pretraining** → Can use **both** encoder & decoder objectives or custom training (e.g., T5 replaces text spans with a single mask).

✅ **Best for tasks that require generating text based on an input sentence!**

---

## **2️⃣ What Are Seq2Seq Models Used For?**

📌 **Ideal for:**

✔️ **Summarization** → Condensing long text into key points.

✔️ **Translation** → Converting text between languages.

✔️ **Generative Question Answering** → Answering questions by generating responses.

---

## **3️⃣ Popular Seq2Seq Models**

🔥 **Well-known encoder-decoder Transformers:**

- **📄 BART** → Advanced text-to-text generation.
- **🌍 mBART** → Multilingual BART for translation.
- **🔁 Marian** → Specialized in machine translation.
- **🧠 T5** → Converts **every NLP task into a text-to-text format**.
