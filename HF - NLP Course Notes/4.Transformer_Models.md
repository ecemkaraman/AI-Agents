### **ğŸ“¥ Encoder-Only Transformer Models**

---

## **1ï¸âƒ£ What Are Encoder Models?**

ğŸ’¡ **Encoder models** use only the **encoder** part of a Transformer model.

ğŸ“Œ **Key Features:**

- **ğŸ”„ Bi-Directional Attention** â†’ Can see **both left & right** context.
- **ğŸ“– Auto-Encoding Models** â†’ Focus on **understanding** entire sentences.
- **âš™ï¸ Training Method** â†’ Corrupt input text (e.g., **masking words**) â†’ Train model to **reconstruct** it.

âœ… **Best for tasks that require full-sentence comprehension!**

---

## **2ï¸âƒ£ What Are Encoder Models Used For?**

ğŸ“Œ **Ideal for:**

âœ”ï¸ **Sentence Classification** â†’ Sentiment analysis, spam detection.

âœ”ï¸ **Named Entity Recognition (NER)** â†’ Identifying people, locations, organizations.

âœ”ï¸ **Word-Level Classification** â†’ Part-of-Speech (POS) tagging.

âœ”ï¸ **Extractive Question Answering** â†’ Finding answers from a given text.

---

## **3ï¸âƒ£ Popular Encoder Models**

ğŸ”¥ **Well-known encoder-based Transformers:**

- **ğŸ“– BERT** â†’ Original bi-directional Transformer.
- **ğŸ§  RoBERTa** â†’ Improved BERT (longer training, more data).
- **âš¡ DistilBERT** â†’ **Smaller, faster, cheaper** version of BERT.
- **ğŸ§© ALBERT** â†’ Efficient BERT variant (parameter sharing).
- **âš™ï¸ ELECTRA** â†’ **Replaces words instead of masking** for better efficiency.

---

### **ğŸ“¤ Decoder-Only Transformer Models**

---

## **1ï¸âƒ£ What Are Decoder Models?**

ğŸ’¡ **Decoder models** use only the **decoder** part of a Transformer model.

ğŸ“Œ **Key Features:**

- **ğŸ”„ Auto-Regressive** â†’ Predicts text **one word at a time**, only seeing past words.
- **ğŸ“ Next-Word Prediction** â†’ Trained by **guessing the next word** in a sentence.

âœ… **Best for tasks that require text generation!**

---

## **2ï¸âƒ£ What Are Decoder Models Used For?**

ğŸ“Œ **Ideal for:**

âœ”ï¸ **Text Generation** â†’ Writing articles, stories, or code.

âœ”ï¸ **Chatbots & Conversational AI** â†’ Open-domain conversations.

âœ”ï¸ **Autoregressive Completion** â†’ Completing user input (e.g., autocomplete).

---

## **3ï¸âƒ£ Popular Decoder Models**

ğŸ”¥ **Well-known decoder-based Transformers:**

- **ğŸ›  GPT** â†’ Original auto-regressive Transformer.
- **ğŸš€ GPT-2** â†’ Larger, better version of GPT.
- **ğŸ“ˆ Transformer XL** â†’ Long-context text generation.
- **ğŸ”„ CTRL** â†’ Controlled text generation with predefined prompts.

---

### **ğŸ”„ Sequence-to-Sequence (Seq2Seq) Transformer Models**

---

## **1ï¸âƒ£ What Are Sequence-to-Sequence Models?**

ğŸ’¡ **Seq2Seq models (encoder-decoder models)** use **both** the encoder & decoder components of a Transformer.

ğŸ“Œ **Key Features:**

- **ğŸ“¥ Encoder** â†’ **Processes** the full input sentence.
- **ğŸ“¤ Decoder** â†’ **Generates output** word-by-word, attending only to previous words.
- **ğŸ“ Advanced Pretraining** â†’ Can use **both** encoder & decoder objectives or custom training (e.g., T5 replaces text spans with a single mask).

âœ… **Best for tasks that require generating text based on an input sentence!**

---

## **2ï¸âƒ£ What Are Seq2Seq Models Used For?**

ğŸ“Œ **Ideal for:**

âœ”ï¸ **Summarization** â†’ Condensing long text into key points.

âœ”ï¸ **Translation** â†’ Converting text between languages.

âœ”ï¸ **Generative Question Answering** â†’ Answering questions by generating responses.

---

## **3ï¸âƒ£ Popular Seq2Seq Models**

ğŸ”¥ **Well-known encoder-decoder Transformers:**

- **ğŸ“„ BART** â†’ Advanced text-to-text generation.
- **ğŸŒ mBART** â†’ Multilingual BART for translation.
- **ğŸ” Marian** â†’ Specialized in machine translation.
- **ğŸ§  T5** â†’ Converts **every NLP task into a text-to-text format**.
