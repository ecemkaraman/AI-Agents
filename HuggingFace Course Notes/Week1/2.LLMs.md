### **1Ô∏è‚É£ What is a Large Language Model (LLM)?**

An **LLM** is an AI model trained on vast text datasets to:

- **Understand & generate** human-like language.
- **Recognize patterns, structure, & nuances** in text.
- **Predict the next token** based on prior context.

üîπ **Core Structure:** Millions to billions of parameters, typically built on the **Transformer architecture** (introduced by Google‚Äôs BERT in 2018).

![Original Transformer Architecture: Encoder + Decoder ](https://machinelearningmastery.com/wp-content/uploads/2021/08/attention_research_1.png)

Original Transformer Architecture: Encoder + Decoder 

---

### **2Ô∏è‚É£ 3 Types of Transformer-Based Models**

1. **Encoders (Understanding-focused)**
    - **Function:** Converts text into dense representations (embeddings).
    - **Example:** **BERT (Google)**
    - **Use Cases:** Text classification, semantic search, named entity recognition.
    - **Typical Size**: Millions of parameters
2. **Decoders (Generation-focused)**
    - **Function:** Predicts & generates text, token by token.
    - **Example:** **Llama (Meta)**
    - **Use Cases:** Chatbots, code generation, text completion.
    - **Typical Size**: Billions of parameters
3. **Seq2Seq (Encoder-Decoder)**
    - **Function:** Encodes input ‚Üí creates context ‚Üí decodes into output.
    - **Example:** **T5, BART**
    - **Use Cases:** Translation, summarization, paraphrasing.
    - **Typical Size**: Millions of parameters

üîπ **Most LLMs today are decoder-based with billions of parameters.**

---

### **3Ô∏è‚É£ Popular LLMs in Use**

| **Model** | **Provider** |
| --- | --- |
| **DeepSeek-R1** | DeepSeek |
| **GPT-4** | OpenAI |
| **Llama 3** | Meta |
| **SmollLM2** | Hugging Face |
| **Gemma** | Google |
| **Mistral** | Mistral |

---

### **4Ô∏è‚É£ How LLMs Process & Predict Text**

- **Tokenization:** Breaks text into **tokens** (subwords, characters, or words).
    - **Token** = unit of info an LLM works with
    - Example: "interesting" ‚Üí ["interest", "ing"]
    - Different tokenizers output different token splits based on algorithm, vocabulary size, and encoding rules.
        
        üîπ¬†**Example: "Transformers are amazing!"**
        
        | **Tokenizer** | **Output Tokens** |
        | --- | --- |
        | **BPE (GPT-4)** | `["Transform", "ers", " are", " amazing", "!"]` |
        | **WordPiece (BERT)** | `["Transformers", " are", " amazing", "!"]` |
        | **Unigram (T5, Llama)** | `["‚ñÅTransformers", "‚ñÅare", "‚ñÅamazing", "!"]` |
        | **SentencePiece (DeepSeek-R1)** | `["‚ñÅTrans", "formers", "‚ñÅare", "‚ñÅamazing", "!"]` |
    - **Main Tokenizer Types:** The core¬†**tokenizer types**¬†are public, but specific AI Models may use fine tuned versions of them (e.g. BPE is an algo, but GPT models use a custom version of BPE)
        - **Byte Pair Encoding (BPE)** ‚Äì Merges most frequent character pairs iteratively, balancing vocabulary size and flexibility.
        - **WordPiece** ‚Äì Similar to BPE but optimizes for likelihood in a probabilistic model, used in BERT.
        - **Unigram Language Model** ‚Äì Starts with full vocabulary and removes less probable tokens iteratively, used in T5.
        - **SentencePiece** ‚Äì Works directly on raw text, supports BPE and Unigram, handles whitespace naturally.
        - **Character-Level Tokenizer** ‚Äì Splits text into individual characters, useful for highly flexible or low-resource tasks.
- **Next-Token Prediction:** Predicts the **next token** based on prior tokens. ‚ÜíCore LLM Principle
- **Autoregressive Nature:** Each predicted token feeds into the next step, continuing until an **End of Sequence (EOS) token** is generated.
    - An LLM will decode text until it reaches the EOS
- Each LLM has some model-specific special tokens‚Üíused to open/close the structured components ‚Üí **EOS Tokens Vary by Model:**
    
    
    | **Model** | **Provider** | **EOS Token** |
    | --- | --- | --- |
    | **GPT-4** | OpenAI | `<|endoftext|>` |
    | **Llama 3** | Meta | `<|eot_id|>` |
    | **DeepSeek-R1** | DeepSeek | `<|end_of_sentence|>` |
    | **SmollLM2** | Hugging Face | `<|im_end|>` |
    | **Gemma** | Google | `<end_of_turn>` |

---

### **5Ô∏è‚É£ How LLMs Generate Text (Decoding Strategies)**

Once tokenized, the model assigns probabilities to the next token and selects one using different **decoding strategies**:

- **Greedy Decoding:** Always picks the highest probability token.
- **Beam Search:** Keeps top-k sequences, expands each step, scores based on cumulative probability, prunes lower-ranked, repeats until EOS for balanced optimality.
- **Top-K Sampling:** Chooses from the top K most probable tokens.

üîπ **Example:** SmollLM2 decodes text until it reaches `<|im_end|>` (EOS token).

---

- **Attention mechanism:** Identifies key tokens, prioritizes relevance, enhances next-token prediction efficiency. ‚Üí *Core aspect of transformers*
- **Scaling advancements:** Improves attention span, memory, enables longer sequence handling, optimizes context retention.
- **Context length:** Max number of tokens an LLM can process & the max attention span it has.
- **Prompting:** Guides model output, structures input effectively, maximizes response accuracy.
- **Training:**
    - **Pre-training: (Unsupervised)** Predicts next token, learns structure and generalizes
    - **Fine-tuning: (Supervised)** Specializes models to perform specific tasks, refines performance
- **LLM Deployment options:** Run locally (requires hardware) or use a Cloud/API
- **AI Agent integration:** Interprets input, maintains context, selects actions, executes structured plans. ‚Üí LLM is a key component of the agent (brain)

---

‚úÖ **Summary:**

- **LLMs** are advanced AI models trained on vast text data to generate and understand language.
- **Transformer-based architectures** (Encoders, Decoders, Seq2Seq) define their function.
- **Tokenization & EOS tokens** control input/output structuring.
- **Decoding strategies** impact how LLMs generate responses.

These models **power chatbots, search engines, translation tools, and AI assistants** worldwide. üöÄ
